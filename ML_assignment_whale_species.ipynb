{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c684ed",
   "metadata": {},
   "source": [
    "# Image Classification ML Assignment \n",
    "\n",
    "The goal of this assignment is to learn fundamentals of machine learning and image analysis. You will analyze images from Happy Whale, an organization that tracks whales globally, and modify a classifier for these images. \n",
    "\n",
    "Part 1: Walk through (Due Feb 27)\n",
    "- In class, we will walk through the provided code\n",
    "- This script has several functions that are written for you. For part 1, please do NOT modify any code unless it specifies to change it.\n",
    "- Answer the questions as you go through the code\n",
    "\n",
    "Part 2: Make modifications (Due Feb 27)\n",
    "- Change model parameters or other model aspects at least 3 times \n",
    "- Describe what happened when you made these changes\n",
    "\n",
    "Part 3: Create a team, get inspired and try a new approach to your model (Due March 6)\n",
    "- Have a group meeting and record your meeting with audio or video (you can record with zoom!)\n",
    "    - Start by discussing part 2 of the assignment:\n",
    "        - What approaches you each took to change the ML model in part 2\n",
    "        - What are similarities and differences between your approaches \n",
    "        - What types of changes improved model performance\n",
    "        - What types of changes made model performance worse\n",
    "        - Based on the whole group's work, what would y'all want to do next in terms of model optimization?\n",
    "    - Next, look at code and approaches from the [Happy Whales competition leaderboard](https://www.kaggle.com/competitions/happy-whale-and-dolphin/leaderboard).\n",
    "        - Is there any strategy that someone used that you could implement? \n",
    "            - Ex: learning rate value, # epochs, transformation strategy, or more complex changes to the models\n",
    "        - Are there strategies that you are interested in learning about?\n",
    "    - Lastly, make a plan for 1 more round of model optimization\n",
    "        - Assign each member something to attempt for training the model. Use your cumulative experience to decide the most important things to test. \n",
    "        - Group members can make as small as changing the value for a parameter or as big as changing the model.\n",
    "- Have each member complete their test\n",
    "- Share results with each other\n",
    "- Write a 1-3 paragraph summary on your group's attempts and recommendations moving forward\n",
    "- For Part 3: Turn in the audio/video recording of the group meeting, each member's code (uploaded to their original git repo), and the team's 1-3 paragraph summary over canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac204c-cb25-4911-91d7-aff35b450a47",
   "metadata": {},
   "source": [
    "### Your choices on data\n",
    "You can chose between 3 datasets for this assignment. This jupyter notebook is set up using the Species dataset. If you change the dataset, you will need to change the number of classes parameter for the model.\n",
    "- Species = you will classify images by species (gray, killer, beluga, humpback, false killer, and common dolphin)\n",
    "  - File path: /projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species\n",
    "- Species_all = you will classify images by species (minke, gray, killer, beluga, humpback, false killer, and bottlenose and common dolphin). There are a high number of images for bottlenose dolphins and minke whales which leads to this dataset being quite large. Due to the size of the dataset, it has a longer training time than the \"Species\" dataset.\n",
    "  - File path: /projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species_all\n",
    "  - Num classes: 8\n",
    "- Individuals = you will classify images by individuals. This is a much more challenging classification task because you have different individuals from different species. However, this was the original goal of the Happy Whale competition, so try challenging yourself to this important task!\n",
    "  - File path: /projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/individuals\n",
    "  - Num classes: 110\n",
    "\n",
    "You are NOT graded by the success of your model, so you are free to train the model on whichever dataset is more interesting to you. However, **everyone on your team for part 3 should be using the same dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbaa389-a60c-4b4f-b059-bcb93ebae159",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61dc4335-2123-4a48-b896-2f943f1a20b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;66;03m#comment this out if you are not using weights and biases\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;66;03m#comment this out if you are not using weights and biases\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Set device to cuda if it's available otherwise default to \"cpu\"\u001b[39;00m\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/wandb/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Configure the logger as early as possible for consistent behavior.\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wb_logging \u001b[38;5;28;01mas\u001b[39;00m _wb_logging\n\u001b[1;32m     23\u001b[0m _wb_logging\u001b[38;5;241m.\u001b[39mconfigure_wandb_logger()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/wandb/sdk/__init__.py:24\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"W&B SDK module.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSettings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/wandb/sdk/wandb_helper.py:6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UsageError\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_util\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_config\u001b[39m(params, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, include\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exclude \u001b[38;5;129;01mand\u001b[39;00m include:\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/wandb/sdk/lib/config_util.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Error\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_yaml\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filesystem\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/wandb/util.py:51\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m     AuthenticationError,\n\u001b[1;32m     46\u001b[0m     CommError,\n\u001b[1;32m     47\u001b[0m     UsageError,\n\u001b[1;32m     48\u001b[0m     WandbCoreNotAvailableError,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m terminput\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filesystem, runid\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dump, dumps\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FilePathStr, StrPath\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/wandb/sdk/lib/filesystem.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IO, Any, BinaryIO, Literal, NewType, TypedDict\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrPath\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[1;32m     21\u001b[0m GlobStr \u001b[38;5;241m=\u001b[39m NewType(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobStr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     23\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/wandb/sdk/wandb_settings.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quote, unquote\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers_pb2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BoolValue, DoubleValue, Int32Value, StringValue\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, ConfigDict, Field\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Self\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/pydantic/__init__.py:421\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n\u001b[1;32m    423\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/importlib/__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/pydantic/fields.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, ClassVar, TypeVar, cast, overload\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mannotated_types\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PydanticUndefined\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/site-packages/annotated_types/__init__.py:299\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Timezone(tz=...) requires a datetime to be aware (or ``tz=None``, naive).\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    ``Annotated[datetime, Timezone(None)]`` must be a naive datetime.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    a symptom of poor design.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     tz: Union[\u001b[38;5;28mstr\u001b[39m, tzinfo, EllipsisType, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfrozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mSLOTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mUnit\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseMetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Indicates that the value is a physical quantity with the specified unit.\u001b[39;49;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;43;03m    It is intended for usage with numeric types, where the value represents the\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;43;03m    Note, however, ``annotated_types`` itself makes no use of the unit string.\u001b[39;49;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/dataclasses.py:1295\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/dataclasses.py:1157\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__hash__\u001b[39m \u001b[38;5;241m=\u001b[39m hash_action(\u001b[38;5;28mcls\u001b[39m, field_list, func_builder)\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# Generate the methods and add them to the class.  This needs to be done\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# before the __doc__ logic below, since inspect will look at the __init__\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# signature.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m \u001b[43mfunc_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_fns_to_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# Create a class doc-string.\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m         \u001b[38;5;66;03m# In some cases fetching a signature is not possible.\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;66;03m# But, we surely should not fail in this case.\u001b[39;00m\n",
      "File \u001b[0;32m/projects/bgmp/shared/Bi625/ML_Assignment/Conda_Envs/HumpbackClassifierEnv/lib/python3.13/dataclasses.py:498\u001b[0m, in \u001b[0;36m_FuncBuilder.add_fns_to_class\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    496\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef __create_fn__(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfns_src\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m return \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m ns \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 498\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m fns \u001b[38;5;241m=\u001b[39m ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__create_fn__\u001b[39m\u001b[38;5;124m'\u001b[39m](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocals)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# Now that we've generated the functions, assign them into cls.\u001b[39;00m\n",
      "File \u001b[0;32m<string>:0\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Importing packages - Please DO NOT alter this box ##\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import DeepLift\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import binary_erosion\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb #comment this out if you are not using weights and biases\n",
    "import random #comment this out if you are not using weights and biases\n",
    "\n",
    "# Set device to cuda if it's available otherwise default to \"cpu\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ec8bf-3b9f-4bb1-ab01-a56bee378aac",
   "metadata": {},
   "source": [
    "### Loading in the dataset\n",
    "\n",
    "By default, this will load in the species dataset, this is the smallest dataset (i.e., it will be relatively fast to train) and has a simple goal (i.e., you will be able to create an accurate model). If you want a challenge, I highly encourage you to try the Species_all (a dataset with 2 additional species) or the individual dataset (a dataset where whales are characterized by individual rather than species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d35c66-3147-442b-9235-b9e3551a06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = \"/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/Whale_species/species\"\n",
    "wandb.__version__\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfa4b0-aecb-4789-98a0-b3568b078bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must turn our images into tensors, resize, and normalize them \n",
    "# We can also add additional transformations to images \n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                #We will start with a model called Resnet18 that is optimized for 224x224 images\n",
    "                                #It is set to a very SMALL size initially so the model will train fast in class\n",
    "                                transforms.Resize([32,32]),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize the data, these are the values that ResNet suggests based on their training data (natural scences)\n",
    "                               ])\n",
    "all_images = datasets.ImageFolder(images, transform)\n",
    "print(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ef932",
   "metadata": {},
   "source": [
    "<font color='magenta'>What is the purpose(s) of transforming image data? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148cd12",
   "metadata": {},
   "source": [
    "The goal with the transformation is that each of the different images can be compared to eachother. For example, if the images are different sizes or if they were taken with different cameras or filters, this step hopefully helps remove some of those biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84ad1a",
   "metadata": {},
   "source": [
    "<font color='green'>The below code completes the image transformations as coded in the above box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3cb05-efe7-4910-8d15-bacf4abe5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = datasets.ImageFolder(images, transform )\n",
    "print(len(all_images))\n",
    "print(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4b32e-f5e6-485c-a381-6e494738a529",
   "metadata": {},
   "source": [
    "Our classification labels will be converted to indices, we can determine which indices is which label using the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0632c03-7c7a-4126-8e78-67560c1d9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_images.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c6341",
   "metadata": {},
   "source": [
    "<font color='green'>The below function is used to randomly select dataset images to evaluate how the transformations altered the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73b9b6-d4dc-4a68-85af-8a0e51aeda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_dataset_image(dataset):\n",
    "    idx = np.random.randint(0, len(dataset))    # take a random sample\n",
    "    img, mask = dataset[idx]                    # get the image and the nuclei masks\n",
    "    f, axarr = plt.subplots(1, 2)               # make two plots on one figure\n",
    "    axarr[0].imshow(img[0], cmap=\"viridis\")                     # show the image, cmap is the color map that the image is being shown in\n",
    "    #axarr[1].imshow(mask[0])                    # show the masks\n",
    "    _ = [ax.axis('off') for ax in axarr]        # remove the axes\n",
    "    print('Image size is %s' % {img[0].shape})\n",
    "    print(img.shape)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76981c44-4733-4dcb-8c97-4f345eb5df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_dataset_image(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d042e5",
   "metadata": {},
   "source": [
    "<font color='magenta'>What is the difference between image size and torch image size? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04154971",
   "metadata": {},
   "source": [
    "The torch image size not only includes the width and height, but also includes the value 3, which stands for number of channels and in this case it is 3 for rgb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424fa16",
   "metadata": {},
   "source": [
    "<font color='magenta'>Add a new transformation to the training data. After your first run of this model during class time, resize the image to 224x224 pixels. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e0c61-6492-4aa0-9a91-c0944eff8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD YOUR TRANSFORMATION HERE\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize([224,224]), # Resize the image as our model is optimized for 224x224 pixels\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) # Normalize the data, these are the values that ResNet suggests based on their training data (natural scences))\n",
    "\n",
    "all_images = datasets.ImageFolder(images, transform )\n",
    "show_random_dataset_image(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86826ff-a50f-465b-b3ea-cb69143f0f07",
   "metadata": {},
   "source": [
    "<font color='magenta'>How did your transformation impact the images? (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc3839",
   "metadata": {},
   "source": [
    "This transformation made this much more clearer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aadcf0-a40e-4c91-ba1e-b90c886112d4",
   "metadata": {},
   "source": [
    "### Creating datasets used in training and testing\n",
    "\n",
    "After loading in the data, we separate the data into training, validation, and testing datasets. The training data is inputted into the model during the training phase, and the model's predictions from this data is used to modulate the weights of the model. The validation data is used while the model trains, but the model is in a evaluation rather than training mode. The validation data gives us a real time view on how accurately the model is predicting our labels. The testing data is used after the model has completed training and tells us how successful our model will be with a novel dataset.\n",
    "\n",
    "Once creating these 3 datasets, we prepare dataloaders which are used to load the images into our models. These images are loaded in batches into the model (a parameter called batch size). Because our dataset is imbalanced, we use a weighted random sampler to select images for our batches in the dataloader. Therefore to create our dataloader, we start by creating a function to get weights, calculating weights, initiating our weighted random sampler, and then creating our dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881500f",
   "metadata": {},
   "source": [
    "<font color='green'>The below code eastablishes the data split between training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a7944-26ec-4d9e-af09-7b7a375a58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(all_images))\n",
    "val_size = int(0.15 * len(all_images))\n",
    "test_size = len(all_images) - (train_size + val_size)\n",
    "print(train_size, val_size, test_size)\n",
    "assert train_size + val_size + test_size == len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5770b4-a029-4dc6-b948-c0b16b4a6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(all_images, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35d904",
   "metadata": {},
   "source": [
    "<font color='green'>The below function is used to get weights for the image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879d27a-063d-4ff0-9769-68b13cf5787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_weights(subset,full_dataset):\n",
    "    ys = np.array([y for _, y in subset])\n",
    "    counts = np.bincount(ys)\n",
    "    label_weights = 1.0 / counts\n",
    "    weights = label_weights[ys]\n",
    "\n",
    "    print(\"Number of images per class:\")\n",
    "    for c, n, w in zip(full_dataset.classes, counts, label_weights):\n",
    "        print(f\"\\t{c}:\\tn={n}\\tweight={w}\")\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04213e8-cece-448f-b0cf-365b5e491bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weights = _get_weights(train_set,all_images)\n",
    "train_sampler = WeightedRandomSampler(train_weights, len(train_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1a223",
   "metadata": {},
   "source": [
    "<font color='magenta'>What is the impact of the class weight on how data is loaded into the model? What does it mean if a class has a low vs high weight? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5fb2a-6c5b-4b1d-9a86-fefdf5114706",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "  \n",
    "Check out this resource: https://towardsdatascience.com/demystifying-pytorchs-weightedrandomsampler-by-example-a68aceccb452 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0258c",
   "metadata": {},
   "source": [
    "The weights indicate the probability of an image being selected. Classes that have more pictures that belong to that class will have smaller weights. That is why humpback whales have the lowest weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00a825-883d-4dbf-803e-5474a5929c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=48, drop_last=True, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_set, batch_size=48, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=48, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c40d2e9-d87a-484d-9aad-57b8dfcc5804",
   "metadata": {},
   "source": [
    "### Picking hyperparameters\n",
    "\n",
    "We can set a few hyperparameters that we decide and can tune. These include:\n",
    "\n",
    "- Learning rate = how much to update our model's parameters at each batch/epoch\n",
    "- batch size = the number of data samples to pass through the network before updating it\n",
    "    - If you change the batchsize below, make sure you ALSO change it in the commands above for the DataLoader\n",
    "- Number of epochs = # of times to iterate over the dataset in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c562849-5268-49a5-9a64-7f70ee09114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "batchsize=48\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30295e",
   "metadata": {},
   "source": [
    "<font color='magenta'>How would model training be impacted if we decreased our learning rate? What about if we increased it? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6b641",
   "metadata": {},
   "source": [
    "Increasing the learning rate might take the model longer as the model's parameters will update much more frequently during the learning process. If we decrease the learning rate, it will run faster, but maybe not have the same amount of predicting power than if it were more frequently updating those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead7d49-2aff-46c6-980a-a33ff6209841",
   "metadata": {},
   "source": [
    "### Training our model\n",
    "\n",
    "For part 1, we are going to use the model ResNet18. This is a published model for imaging data. Resnet18 is built from 4 residual blocks with two convolutional layers (Conv2d). Each convolutional layer is followed by a batch normalization step (BatchNorm2D) and a ReLU activation layer (ReLU). There is also a shortcut connection which adds input to output and allows the network to learn residual mapping.\n",
    "\n",
    "I provided the complete code for the ResNet18 model. In the future, if you want to use resnet18, you can load it simply by running `resnet18_model = torchvision.models.resnet18(weights = False, progress  = True, num_classes=XXX)`. We will look at the full code for the model so you can see how this model was created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66640e-3eae-4a43-ba04-a9223f92641e",
   "metadata": {},
   "source": [
    "<font color='green'>The below code builds the residual blocks that ResNet18 uses in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414ef8b-e863-4485-b6ce-8b4305fe87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we create the basic block that will be used in our residual net\n",
    "class BasicBlock(nn.Module):\n",
    "    # Initializing method for the basic block (It's OOP!)\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        ## Conv1: convolution layer, batch normalization, ReLU activation\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        ## Conv2: convolution layer, batch normalization\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        ## Shortcut connection: adds input to output\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    # The forward method calls on each layer\n",
    "    def forward(self, x):\n",
    "        ## Conv1: convolution layer, batch normalization, ReLU activation\n",
    "        out = self.conv1(x) \n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        ## Conv2: convolution layer, batch normalization\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        ## Shortcut connection\n",
    "        out += self.shortcut(x)\n",
    "        ## Final activation\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661765f-3551-4075-87a6-f159d2a4d0ad",
   "metadata": {},
   "source": [
    "<font color='green'>The below code builds and initializes the ResNet18 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311eb11-a910-46e6-813c-8e96fcdc2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, we put together these building blocks and create our residual net\n",
    "class ResNet18(nn.Module):\n",
    "    # specify the number of classes that we are predicting\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(ResNet18, self).__init__()\n",
    "        # In channels = Num pixels in H + Num pixels in W\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # First convolution set (convolution, batch norm, relu)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Our building blocks\n",
    "        # The numbers correspond to the matrix shape\n",
    "        ### We increase the number of filters/channels (i.e., the first number) as we go\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "\n",
    "        # Average pooling \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Final layer that makes the classification\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution set (convolution, batch norm, relu)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # Our 4 building blocks\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # Final layer that makes the classification\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "## Getting our model and transferring it to the GPU\n",
    "model = ResNet18().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576148d1-bbc1-4272-988f-d13075ea21f1",
   "metadata": {},
   "source": [
    "<font color='green'>The below code eastablishes the loss function and optimizer used in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cddabe4-9dbc-4a33-bb01-106e94c9dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700e6a2",
   "metadata": {},
   "source": [
    "<font color='green'>The below functions are used to train, evaluate, and test the Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebff257",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"BGMP_HappyWhale\",\n",
    "    name=\"Makayla-HappyWhale-SpeciesDataset-hyperparameters\", ### Update with your name!\n",
    "    config={\"learning rate\":0.001,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"Species\",\n",
    "        \"epochs\":5}\n",
    ")\n",
    "\n",
    "num_epochs = epochs\n",
    "train_losses, train_acc_list, val_losses, val_acc_list = [], [], [],[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Setting model to \"training mode\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    batch=0\n",
    "\n",
    "    # For each batch of data within the loader\n",
    "    for inputs, labels in train_loader:\n",
    "        # Send our input images and their labels to the GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Inputting our training images into the model\n",
    "        # and Predicting the image classification label\n",
    "        outputs = model(inputs)\n",
    "        # Figuring out the loss from our predictions\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Compute gradients (aka backward pass)\n",
    "        loss.backward()\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adding the loss to our running sum\n",
    "        # Loss is calculated for each batch within an epoch\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        batch+=1\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | Batch #{batch} | Batch Accuracy {(correct/total)*100:.2f}%')\n",
    "\n",
    "\n",
    "    # Getting metrics for our training pass \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    # Switching our model to \"evaluation mode\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    # Disable gradient calculation b/c we are evalulating the model\n",
    "    with torch.no_grad():\n",
    "        # Load in batches of our validation data\n",
    "        for inputs, labels in val_loader:\n",
    "            # Send test images and labels to the GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Predict the image classification label\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            # Figuring out how many predicted labels = true labels\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Figuring out the loss from our predictions\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Adding the loss to our running sum\n",
    "            # Loss is calculated for each batch within an epoch\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    # Getting our accuracy from our test data\n",
    "    val_acc = 100. * correct / total\n",
    "    val_acc_list.append(val_acc)\n",
    "    # Getting the loss from our test data\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Val Loss: {val_loss:.2f}%')\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"validation_accuracy\": val_acc, \"validation_loss\": val_loss, \"train_loss\":train_loss})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641b2c5",
   "metadata": {},
   "source": [
    "#### For our class training, run the below model and consider the success of the model training. \n",
    "\n",
    "<font color='magenta'>What is your starting accuracy and final accuracy? What happened to the model to change the accuracy from epoch 1 to 5? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d79c89",
   "metadata": {},
   "source": [
    "The starting accuracy was ~19% to start. As the model started, the batch accuracy percent initially dips before increasing after the 2nd batch. On epoch 2 batch 20 we are up to over 60%. The run ended with a validation accuracy of 66% with a validation loss of 0.83 and a training loss of 0.8. The final batch accuracy was 70.78%. Overall, as the model learned through each epoch, the model accuracy improved. The most improvement occured during the first few batches of the first epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c935f-6314-4791-9e16-9e4f4d489790",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_epochs), val_acc_list, color = \"magenta\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"Valiation accuracy\")\n",
    "plt.show()  \n",
    "\n",
    "plt.plot(range(num_epochs), val_losses, color = \"purple\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"Valiation loss\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec6595",
   "metadata": {},
   "source": [
    "<font color='green'>After running the model and calculating the overall accuracy, we can examine the data via a confusion matrix which highlights the accuracy by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test dataset\n",
    "def predict(model, dataset):\n",
    "    dataset_prediction = []\n",
    "    dataset_groundtruth = []\n",
    "    model = model\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in dataset:\n",
    "            inp = x[None]\n",
    "            y_pred = model(inp)\n",
    "            dataset_prediction.append(y_pred.argmax().cpu().numpy())\n",
    "            dataset_groundtruth.append(y_true)\n",
    "    \n",
    "    return np.array(dataset_prediction), np.array(dataset_groundtruth)\n",
    "            \n",
    "    # create seaborn heatmap with required labels\n",
    "    ax=sns.heatmap(cm, annot=annot, fmt='', vmax=30, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Plot confusion matrix \n",
    "# orginally from Runqi Yang; \n",
    "# see https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7\n",
    "def cm_analysis(y_true, y_pred, title, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    labels = ['Beluga','Common dolphin', 'False killer whale', 'Fin whale', 'Gray whale','Humpback whale']\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "                              \n",
    "    x_axis_labels = ['Beluga','Common dolphin', 'False killer whale', 'Fin whale', 'Gray whale','Humpback whale'] # labels for x-axis\n",
    "    y_axis_labels = ['Beluga','Common dolphin', 'False killer whale', 'Fin whale', 'Gray whale','Humpback whale'] # labels for y-axis\n",
    "    ax=sns.heatmap(cm, annot=annot, fmt='', vmax=30, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cmap = \"viridis\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "## This plot only contains the test set of data\n",
    "## The test set of data has not been seen by the model yet\n",
    "y_pred, y_true = predict(model, test_set)\n",
    "\n",
    "print(y_true)\n",
    "cm_analysis(y_true, y_pred, \"Confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ce810",
   "metadata": {},
   "source": [
    "### What features did the model use to make decisions?\n",
    "\n",
    "Based on the confusion matrices and validation loss, it is clear that the model is learning. To determine the features that are involved in decision making, there are a few possible approaches. One options is shown below, integrated gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1a543",
   "metadata": {},
   "source": [
    "#### Understanding feature importance using Integrated Gradients\n",
    "\n",
    "<font color='green'>Integrated gradients is an attribution method. Attribution methods score the input data based on the prediction that the model makes using scores for each feature. The gradient is the signal that tells the network how much to increase or decrease a certain weight in the network during backpropogation. These gradients are overlaid onto the images to showcase the regions on the images that influence the weights.\n",
    "\n",
    "Run the below code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae68173",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_ig(idx, \n",
    "                 _train_dataset=train_set, \n",
    "                 _test_dataset=test_set):\n",
    "        \n",
    "    # Get corresponding input and target tensors:\n",
    "    input_tensor = _test_dataset[idx][0].unsqueeze(0)\n",
    "    input_tensor.requires_grad = True\n",
    "    target = _test_dataset[idx][1]\n",
    "    \n",
    "    # We will use the IntegratedGradients algorithm:\n",
    "    algorithm = IntegratedGradients(model)\n",
    "    \n",
    "    # First we clear the gradients from the model:\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Run attribution:\n",
    "    attr_ig, delta = algorithm.attribute(input_tensor,\n",
    "                                          target=target,\n",
    "                                          baselines=input_tensor * 0,\n",
    "                                          return_convergence_delta=True\n",
    "                                        )\n",
    "    \n",
    "    # Integrated Gradients:\n",
    "    attr_ig = np.transpose(attr_ig[0].cpu().detach().numpy(), (1, 2, 0))\n",
    "    \n",
    "    # Original image:\n",
    "    original_image = np.transpose((_test_dataset[idx][0].detach().numpy() * 0.5) + 0.5, (1, 2, 0))\n",
    "    \n",
    "    print(\"Annotated whale species\", test_true[idx])\n",
    "    plt.imshow(original_image)\n",
    "    \n",
    "    \n",
    "    print(\"Predicted whale species\", test_pred[idx])\n",
    "\n",
    "\n",
    "    # This visualises the attribution of labels to \n",
    "    viz.visualize_image_attr(attr_ig, \n",
    "                             original_image, \n",
    "                             method=\"blended_heat_map\",\n",
    "                             sign=\"all\",\n",
    "                             show_colorbar=True, \n",
    "                             title=\"Overlayed Integrated Gradients\")\n",
    "\n",
    "test_pred, test_true = predict(model, test_set)\n",
    "\n",
    "visualize_ig(0)\n",
    "visualize_ig(8)\n",
    "visualize_ig(20)\n",
    "visualize_ig(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c361ff58-3347-42b1-bad0-da7206fa26d3",
   "metadata": {},
   "source": [
    "<font color='magenta'>Does the integrated gradient show what types of features the model used to classify the images? Do you notice any potential problems? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b40850",
   "metadata": {},
   "source": [
    "I think the gradietns seem to be identifying the fin outlines from the dark shapes in each picture. Then it uses the fin shape to identify. However, in the first picture, it seems like the model may have identified a group of trees in the background as the whale instead of the part of the image which actually shows the whale. To be fair, the part of the image with the whale is very small as the whale is barely poiking out of the water and that image might be extremely difficult to ID the species. Regardless, the trees should not be representative of the whale species in that first picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25173115",
   "metadata": {},
   "source": [
    "## Part 2 \n",
    "\n",
    "Now that you have a sense for how to train a machine learning model, your task is to improve the success of our classification model.\n",
    "\n",
    "**In three separate training events, make one (or more) modifications to the model training process, re-train the model, and report out your changes and whether they increased model accuracy. Why did you chose your particular change (curiousity, hypothesis, ect.)? Do you have ideas why your modifications were/were not successful?**\n",
    "\n",
    "Types of changes that you can make include:\n",
    "- [Using the pre-trained weights for ResNet18 (transfer learning)](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) \n",
    "- Changing hyperparameters (learning rate, batch size)\n",
    "- [Changing transformation approach](https://docs.pytorch.org/vision/0.11/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)\n",
    "- [Changing the optimizer](https://docs.pytorch.org/docs/stable/optim.html)\n",
    "- [Changing the model to a different pre-made model architecture](https://docs.pytorch.org/vision/main/models.html)\n",
    "- Modifying the ResNet18 model\n",
    "- Coming up with a whole new model!\n",
    "\n",
    "If you decide to change any lines of code that are not in the below box, please add them to the box (DO NOT MODIFY ANY CODE ABOVE THIS MARKDOWN BOX - IT MAY BREAK PART 1). \n",
    "\n",
    "If your changes are ambitious, the code may take hours to run. If this is the case, please consider turning this notebook into a python script and running it via sBATCH. \n",
    "\n",
    "**Please note, you will NOT be graded by the success of your classifiers! It is okay if you are unable to improve the accuracy of the classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3766c2d-cb6d-429b-943b-0a95d4c6cd2d",
   "metadata": {},
   "source": [
    "Just as a quick reminder, here is the confusion matrix and graph results from the first model. I can use these to compare my model 1 to these.\n",
    "![`model1_1 accuracy`](./model1_1/model1_1_accuracy.png)\n",
    "![`model1_1 val loss`](./model1_1/model1_1_validation_loss.png)\n",
    "![`model1_1 confusion matrix`](./model1_1/ConfusionMatrix1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa542f1c",
   "metadata": {},
   "source": [
    "#### Your Model 1 (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c4d40",
   "metadata": {},
   "source": [
    "<font color='magenta'>Describe your changes to the model training process (either changing something about the model, a hyperparameter, how data was inputted into the model, or another aspect) and why you selected those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd7f75-1b02-4911-ba49-e22a0d0e9953",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "All 3 of my models were run in python scripts and can be found on my repository. For the first 3 models, I kept the resolutions of the images 32x32. (All files in respect to each of these models can be found in the directories labeled model1, model2, model3, and model4 on the repository.) This is because I expect these to run a little faster than a more higher resolution photo like 244x244.\n",
    "In the first model, I set batch size to 48, learning rate to 0.001, and epoch to 5. This should mimic what I did in part 1, but with a lower resolution, so I expect it to be a little less computationally heavy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0ff2b-5ae3-46ea-bea9-bbc987edb126",
   "metadata": {},
   "source": [
    "<font color='magenta'>Were your changes successful? Do you have ideas why your changes were successful or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd7a93-7cd7-4365-b25d-526c3ba6d6e4",
   "metadata": {},
   "source": [
    "This was successful in the sense that it was faster (9min instead of over an hour), which was what I was aiming for when reducing the resolution. This also resulted in a lower validation accuracy of 64% rather than 73%.\n",
    "\n",
    "Model 1 Accuracy \n",
    "\n",
    "![model 1 accuracy](./model1/model1_accuracy.png)\n",
    "\n",
    "Model 1 Validation Loss\n",
    "\n",
    "![model 1 validation loss](./model1/model1_validation_loss.png)\n",
    "\n",
    "Model 1 Conusion Matrix\n",
    "\n",
    "![model 1 confmatrix](./model1/ConfusionMatrix1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a92805-56dc-4576-9e49-d4cc142e89b3",
   "metadata": {},
   "source": [
    "This model seems to also have higher true predictions for all whale species compared to the first model with the exception of the false killer whale. It is interesting that not all species were more strongly predicted with one model over the other when this model seemed to be doing so strongly. These predictions could be slightly different the next time I run each of the models as well so maybe that opens up some variability that exists for each time the model is run. The general shape of the loss and accuracy plots look similar between the two models, but running more epochs might allow for more fine tuning of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34412515",
   "metadata": {},
   "source": [
    "#### Your Model 2 (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836b385",
   "metadata": {},
   "source": [
    "<font color='magenta'>Describe your changes to the model training process (either changing something about the model, a hyperparameter, how data was inputted into the model, or another aspect) and why you selected those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ec5b6-0dc1-49ce-81f5-a68326618d18",
   "metadata": {},
   "source": [
    "Still with a resolution of 32x32, I set the following parameters: batchsize = 60, learning rate = 1e-3, and epochs = 7. Overall, I increased the batch size and the epochs in hopes that this is a more robust, but likely also more computationally heavy model. I expect the accuracies to be higher than those seen in model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9ae55-1425-45a8-9fe3-bd1dd32ba059",
   "metadata": {},
   "source": [
    "<font color='magenta'>Were your changes successful? Do you have ideas why your changes were successful or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4a77f-2605-4bd4-b8c0-138bb6a2a268",
   "metadata": {},
   "source": [
    "\n",
    "Model 2 Accuracy \n",
    "\n",
    "![model 2 accuracy](./model2/model2_accuracy.png)\n",
    "\n",
    "Model 2 Validation Loss\n",
    "\n",
    "![model 2 validation loss](./model2/model2_validation_loss.png)\n",
    "\n",
    "Model 2 Conusion Matrix\n",
    "\n",
    "![model 2 confmatrix](./model2/ConfusionMatrix2.png)\n",
    "\n",
    "This model (#2) was extremely successful. Model 2 had a low resolution, but with a higher batch size and higher epoch number, the model had more opportunities to train. This led to a validation accuracy of 85%. Though I will foreshadow that I re-ran model 2 with the higher resolution later (called model 2_1), but this instead resulted in a lower validation accuracy of 66%, which is well below the first model's validation accuracy. In fact, all of them were like that. The lower resolution versions of the models had higher validation accuracies. I will address this in my final conclusion. \n",
    "\n",
    "In the confusion matrix, we see that this model does better with some species, but not all species compared to the last mode (model 1). It seems to do a little more poorly with identifying common dolfin, fin whale, and gray whale species. \n",
    "\n",
    "There is more epochs for this model so it has more time to learn and adapt as seen in both the accuracy and validation loss graphs compared to the last model too. Overall, this can take time to run so the next model is the first that simply aims at reducing run time instead of improving accuracy. This was initially done to troubleshoot the issue I was having generating the graphs like the model accuracy, validation loss, and confusion matrix figures like those seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a57e5-ad69-4b98-8205-a64eae425db6",
   "metadata": {},
   "source": [
    "#### Your Model 3 (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf44946-eeaa-477f-af36-8dfc1cafd69a",
   "metadata": {},
   "source": [
    "<font color='magenta'>Describe your changes to the model training process (either changing something about the model, a hyperparameter, how data was inputted into the model, or another aspect) and why you selected those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561170e-a8a2-470d-892b-82b025444fc5",
   "metadata": {},
   "source": [
    "Still with a resolution of 32x32, I set the following parameters: batchsize = 10, learning rate = 1e-5, and epochs = 3. This was my first attempt at running a faster version of the model to see how much validation accuracy I would lose (and eventually use this to become a test run for printing the corresponding graphs like the confusion matrix (this is my precursor to model 4, which is my fastest model)). With the goal of reducing time, the batchsize and epochs were lowered. However, lowering the batch size may take longer to parse through all of the images in the dataset as there will be more batches now per epoch. The learning rate was lowered too, but in retrospect, this may needed to have been increased to more agressively adjust when there were fewer epochs, but again, there were more batches per epoch so this may balance out the tradeoffs and lead to a similar percent validation accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c7b9e-1b39-4303-955b-bb1791b42805",
   "metadata": {},
   "source": [
    "<font color='magenta'>Were your changes successful? Do you have ideas why your changes were successful or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cf2191-a0c9-41d1-8aae-94a1db03408e",
   "metadata": {},
   "source": [
    "Model 3 Accuracy \n",
    "\n",
    "![model 3 accuracy](./model3/model3_accuracy.png)\n",
    "\n",
    "Model 2 Validation Loss\n",
    "\n",
    "![model 3 validation loss](./model3/model3_validation_loss.png)\n",
    "\n",
    "Model 3 Conusion Matrix\n",
    "\n",
    "![model 3 confmatrix](./model3/ConfusionMatrix3.png)\n",
    "\n",
    "The changes made to model 3 definitely reduced the runtime, but actually not enough to be faster than model 1. As I mentioned earlier, this may be because setting a batch size of 10 increases the amount of batches needed to  be run for each epoch. The validation accuracy is lower than model 2 as this model has a validation accuracy of 77%, which is less than 86%. However, it is a higher validation accuracy compared to model 1. This again might just be due to more time training and the batch size of 10 actually might increase model training time.\n",
    "\n",
    "We can see that Model 3 has some lower true calls in the confusion matrix than model 2 in all but the fin whale and the gray whale. The shapes in the accuracy and validation loss graphs are pretty similar across all 3 models. For example the accuracy plots have a general increase while the validation loss has a general decrease. This is likely because the number of epochs is too small and we might see more variation with higher epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60816425",
   "metadata": {},
   "source": [
    "<font color='magenta'>Summarize your 3 steps, their success, and what you learned (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1c4f5",
   "metadata": {},
   "source": [
    "I performed my models by running python scripts for each.  I have made a table below for each of the models and epochs. I was able to fine tune model 4 to run very quickly for troubleshooting purposes. \n",
    "\n",
    "All models were run with 60G memory, 10 cores, and patition = gpu. Time was also set to 10 hours, but none of the models ever came close to that time. \n",
    "\n",
    "Here is a comparison table of all 4 of my runs:\n",
    "| Model Number | resolution | batch size| learning rate | epochs | Train Loss | Validation Accuracy | Validation Loss | Runtime|\n",
    "| --- | --- | --- | --- |--- | --- | --- | --- |--- |\n",
    "|Model 1| 32x32 | 48 | 1e-3 | 5 | 0.4408 | 73.96% | 0.6561 | 9m 22s|\n",
    "|Model 1_1*| 244x244 | 48 | 1e-3 | 5 | 0.7991 | 64.84% | 0.90 | 1h 20m 29s|\n",
    "|Model 2| 32x32 |60 | 1e-3 | 7 |0.3251 | 85.8333% |  0.3385 |12m 27s|\n",
    "|Model 2_1| 244x244 |60 | 1e-3 | 7 | 0.6712| 65.83% | 0.8834 |1h 41m 40s|\n",
    "|Model 3| 32x32 |10 | 1e-5 | 3 | 0.4878| 76.82% | 0.6347 |9m 40s|\n",
    "|Model 3_1| 244x244 |10 | 1e-5 | 3 | 0.7681| 65.61% | 0.9384 |50m 38s|\n",
    "|Model 4 |32x32 | 100 | 1e-5 | 2 | 1.051| 29.75%|1.6204|4m 10s| 4m 10s|\n",
    "|Model 4_1 |244x244 | 100 | 1e-5 | 2 | 0.9634| 41.5% | 1.5921 |28m 14s|\n",
    "\n",
    "*completed as part 1 in this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75976b24",
   "metadata": {},
   "source": [
    "It is a little bizarre to me that the higher resolution images have lower validation accuracies. This might mean that more broader types of shapes in each of the images is stronger at predicting the whales. However, this might not be the most reliable with a larger dataset. I would expect that the higher resolution image is more trustworthy to be using for identification, but that it allows the model to identify shapes like background trees and use those to predict the species, which is not reliable and is lowering the validation accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
